from graph import Graph
from scipy import stats
import numpy as np
import itertools
import math
import sys
import os as os
import cPickle as pickle
from matplotlib import pyplot as plt
import seaborn





dir_current = os.getcwd()
dir_name = dir_current + '/figure'


# set up figure
fig = plt.figure()
#ax = fig.add_subplot(111)
ft = 15 # tick font size
fl = 16 # label font size
fs_s = 12 # small text font size
ms = 5 # marker size
lw = 3 # line width size



"""
This code shows how to use BP (sum-product alg)+ gradient ascent opt. to 
get the marginals of each kinases. 

New IC:  Ebind[0,:] = np.array([-5,-3,-3,-3]) 
original IC : Ebind[0,:] = np.array([-1,-2,-1,-1.5])

New omega" omega3 = 2, omegarest = 8 (i.e. freq3 =0.5, freqrest =0.125)
original omega: all omega = 4


"""




def conditional_prob_on(PAstate, Ematrix, Wbind, omega):
	
	"""
	The conditional prob is dependent on the biophysical model

	"""
	if isinstance(omega, float):
		omega = np.array([omega])

	omegainv= omega**-1
	num = 0.0
	den = 0.0
	


	for j in range(0,len(Ematrix)):
		num = num + omegainv[j]*np.exp(-Ematrix[j])*PAstate[j]
		den = den + omegainv[j]*(np.exp(-Ematrix[j])*PAstate[j] + np.exp(-Wbind)*(1-PAstate[j]))
	
	return num / (1 + den)




def sample_generate(margDist, Nsamples):
	""" margDist is a 2-tuple of len(margDist)
		This functions returns a sample of data generated by the marg prob specified
	"""
	xk = np.arange(2)
	margsamples = np.zeros([len(margDist), Nsamples]) 
	for tupleindex in xrange(len(margDist)):
		custm = stats.rv_discrete(name='custm', values=(xk, margDist[tupleindex]))
		margsamples[tupleindex, :]= custm.rvs(size= Nsamples)

	return margsamples


def InfoMaxGD(P1, P2, P34, P123, P45joint, bindingE, bindingW, oall, regC):

	nabla_E = np.zeros(len(bindingE))
	underflowcutoff = 0.001


	freqall= oall**-1
	delP4_3 = np.zeros((2,2), dtype = float)  # d P(x4|x3)/ d epsilon_{34}
	delP5_3 = np.zeros((2,2), dtype = float)  # d P(x5|x3)/ d epsilon_{35}
	delP3_12_1 = np.zeros((2,2,2), dtype = float) # d P(x3|x1 x2) / d epsilon_{13}
	delP3_12_2 = np.zeros((2,2,2), dtype = float) # d P(x3|x1 x2) / d epsilon_{23}
	f124 = np.zeros((2,2,2), dtype = float)
	g124 = np.zeros((2,2,2), dtype = float)
	h124 = np.zeros((2,2,2), dtype = float)
	f125 = np.zeros((2,2,2), dtype = float)
	g125 = np.zeros((2,2,2), dtype = float)
	h125 = np.zeros((2,2,2), dtype = float)



	for tstep in xrange(1):


		delP4_3[1,0] = (-freqall[2]*np.exp(-bindingE[2])*0.0*(1
				+freqall[2]*(1-0.0)*np.exp(-bindingW)))/((1
				+freqall[2]*np.exp(-bindingE[2])*0.0
				+freqall[2]*np.exp(-bindingW)*(1-0.0))**2)

		delP4_3[1,0] = (-freqall[2]*np.exp(-bindingE[2])*1.0*(1
				+freqall[2]*(1-1.0)*np.exp(-bindingW)))/((1
				+freqall[2]*np.exp(-bindingE[2])*1.0
				+freqall[2]*np.exp(-bindingW)*(1-1.0))**2)

		delP4_3[0,:] = - delP4_3[1,:]


		delP5_3[1,0] = (-freqall[2]*np.exp(-bindingE[3])*0.0*(1
				+freqall[2]*(1-0.0)*np.exp(-bindingW)))/((1
				+freqall[2]*np.exp(-bindingE[3])*0.0
				+freqall[2]*np.exp(-bindingW)*(1-0.0))**2)

		delP5_3[1,0] = (-freqall[2]*np.exp(-bindingE[3])*1.0*(1
				+freqall[2]*(1-1.0)*np.exp(-bindingW)))/((1
				+freqall[2]*np.exp(-bindingE[3])*1.0
				+freqall[2]*np.exp(-bindingW)*(1-1.0))**2)

		delP5_3[0,:] = - delP5_3[1,:]



		delP3_12_1[1,0,0] = (-freqall[0]*np.exp(-bindingE[0])*0.0*(1
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*0.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*0.0))**2)


		delP3_12_1[1,1,0] = (-freqall[0]*np.exp(-bindingE[0])*1*(1
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*0.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*0.0))**2)


		delP3_12_1[1,0,1] = (-freqall[0]*np.exp(-bindingE[0])*0.0*(1
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*1.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*1.0))**2)


		delP3_12_1[1,1,1] = (-freqall[0]*np.exp(-bindingE[0])*1.0*(1
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*1.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*1.0))**2)


		delP3_12_1[0,:,:] = - delP3_12_1[1,:,:]




		delP3_12_2[1,0,0] = (-freqall[1]*np.exp(-bindingE[1])*0.0*(1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*0.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*0.0))**2)


		delP3_12_2[1,0,1] = (-freqall[1]*np.exp(-bindingE[1])*1*(1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*1.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*1.0))**2)


		delP3_12_2[1,1,0] = (-freqall[1]*np.exp(-bindingE[1])*0.0*(1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*0.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*0.0))**2)


		delP3_12_2[1,1,1] = (-freqall[1]*np.exp(-bindingE[1])*1.0*(1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*1.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*1.0))**2)


		delP3_12_2[0,:,:] = - delP3_12_2[1,:,:]

		tmp = np.array(list(itertools.product([[0],[1]], repeat = 2)))
		for j in xrange(tmp.shape[0]):
			i1 = tmp[j][0][0]
			i2 = tmp[j][1][0]
			f124[0,i1,i2] = delP4_3[0,0]*P123[0,i1,i2] + delP4_3[0,1]*P123[1,i1,i2]
			f124[1,i1,i2] = delP4_3[1,0]*P123[0,i1,i2] + delP4_3[1,1]*P123[1,i1,i2]
			f125[0,i1,i2] = delP5_3[0,0]*P123[0,i1,i2] + delP5_3[0,1]*P123[1,i1,i2]
			f125[1,i1,i2] = delP5_3[1,0]*P123[0,i1,i2] + delP5_3[1,1]*P123[1,i1,i2]
			
			g124[0,i1,i2] = P34[0,0]*delP3_12_1[0,i1,i2] + P34[1,0]*delP3_12_1[1,i1,i2]
			g124[1,i1,i2] = P34[0,1]*delP3_12_1[0,i1,i2] + P34[1,1]*delP3_12_1[1,i1,i2]
			g125[0,i1,i2] = P35[0,0]*delP3_12_1[0,i1,i2] + P35[1,0]*delP3_12_1[1,i1,i2]
			g125[1,i1,i2] = P35[0,1]*delP3_12_1[0,i1,i2] + P35[1,1]*delP3_12_1[1,i1,i2]

			h124[0,i1,i2] = P34[0,0]*delP3_12_2[0,i1,i2] + P34[1,0]*delP3_12_2[1,i1,i2]
			h124[1,i1,i2] = P34[0,1]*delP3_12_2[0,i1,i2] + P34[1,1]*delP3_12_2[1,i1,i2]
			h125[0,i1,i2] = P35[0,0]*delP3_12_2[0,i1,i2] + P35[1,0]*delP3_12_2[1,i1,i2]
			h125[1,i1,i2] = P35[0,1]*delP3_12_2[0,i1,i2] + P35[1,1]*delP3_12_2[1,i1,i2]

		tmp = np.array(list(itertools.product([[0],[1]], repeat = 4)))
		Ptemp1 = np.zeros((2,2,2,2))
		Ptemp2 = np.zeros((2,2,2,2))
		Ptemp3 = np.zeros((2,2,2,2))
		Ptemp4 = np.zeros((2,2,2,2))
		for j in xrange(tmp.shape[0]):
			i4 = tmp[j][0][0]
			i5 = tmp[j][1][0]
			i1 = tmp[j][2][0]
			i2 = tmp[j][3][0]

			Ptemp1[i4, i5, i1, i2] = (g124[i4,i1,i2]*P125[i5,i1,i2]+ P124[i4,i1,i2]*g125[i5,i1,i2])* \
			(np.log2(P124[i4,i1,i2])+ np.log2(P125[i5,i1,i2])-np.log2(P45joint[i4,i5]) +1)  

			Ptemp2[i4, i5, i1, i2] = (h124[i4,i1,i2]*P125[i5,i1,i2]+ P124[i4,i1,i2]*h125[i5,i1,i2])* \
			(np.log2(P124[i4,i1,i2])+ np.log2(P125[i5,i1,i2])-np.log2(P45joint[i4,i5]) +1)  

			Ptemp3[i4, i5, i1, i2] = (f124[i4,i1,i2]*P125[i5,i1,i2])* \
			(np.log2(P124[i4,i1,i2])+ np.log2(P125[i5,i1,i2])-np.log2(P45joint[i4,i5]) +1)  

			Ptemp4[i4, i5, i1, i2] = (P124[i4,i1,i2]*f125[i5,i1,i2])* \
			(np.log2(P124[i4,i1,i2])+ np.log2(P125[i5,i1,i2])-np.log2(P45joint[i4,i5]) +1) 


		nabla_E[0] = np.sum(np.sum(np.sum(Ptemp1, axis = 0), axis = 0)*P12)
		nabla_E[1] = np.sum(np.sum(np.sum(Ptemp2, axis = 0), axis = 0)*P12)
		nabla_E[2] = np.sum(np.sum(np.sum(Ptemp3, axis = 0), axis = 0)*P12)
		nabla_E[3] = np.sum(np.sum(np.sum(Ptemp4, axis = 0), axis = 0)*P12)



		#nabla_E[0]  = np.dot( np.transpose(P2), np.sum(P1*(np.multiply(g124[0,:,:], np.log(P124[0,:,:]/P4[0]))+g124[0,:,:]) + P1*(np.multiply(g124[1,:,:], np.log(P124[1,:,:]/P4[1]))+g124[1,:,:]), axis = 0))-4*regC*nabla_E[0]#*(nabla_E[0]**2+nabla_E[1]**2+nabla_E[2]**2)
		#nabla_E[1]  = np.dot( np.transpose(P2), np.sum(P1*(np.multiply(h124[0,:,:], np.log(P124[0,:,:]/P4[0]))+h124[0,:,:]) + P1*(np.multiply(h124[1,:,:], np.log(P124[1,:,:]/P4[1]))+h124[1,:,:]), axis = 0))-4*regC*nabla_E[1]#*(nabla_E[0]**2+nabla_E[1]**2+nabla_E[2]**2)	
		#nabla_E[2]  = np.dot( np.transpose(P2), np.sum(P1*(np.multiply(f124[0,:,:], np.log(P124[0,:,:]/P4[0]))+f124[0,:,:]) + P1*(np.multiply(f124[1,:,:], np.log(P124[1,:,:]/P4[1]))+f124[1,:,:]), axis = 0))-4*regC*nabla_E[2]#*(nabla_E[0]**2+nabla_E[1]**2+nabla_E[2]**2)

	return nabla_E
			



def SGD(self, training_data, epochs, mini_batch_size, eta,
			test_data=None):
	"""Train the neural network using mini-batch stochastic
	gradient descent.  The ``training_data`` is a list of tuples
	``(x, y)`` representing the training inputs and the desired
	outputs.  The other non-optional parameters are
	self-explanatory.  If ``test_data`` is provided then the
	network will be evaluated against the test data after each
	epoch, and partial progress printed out.  This is useful for
	tracking progress, but slows things down substantially."""
	if test_data: n_test = len(test_data)
	n = len(training_data)
	for j in xrange(epochs):
		random.shuffle(training_data)
		mini_batches = [
			training_data[k:k+mini_batch_size]
			for k in xrange(0, n, mini_batch_size)]
		for mini_batch in mini_batches:
			[W, allbias] = self.update_mini_batch(mini_batch, eta)
		if test_data:
			print "Epoch {0}: {1} / {2}".format(
				j, self.evaluate(test_data), n_test)
		else:
			print "Epoch {0} complete".format(j)
	return [W, allbias]



def update_mini_batch(self, mini_batch, eta):

	"""Update the network's weights and biases by applying
	gradient descent using backpropagation to a single mini batch.
	The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
	is the learning rate."""
	nabla_b = [np.zeros(b.shape) for b in self.biases]
	nabla_w = [np.zeros(w.shape) for w in self.weights]
	for x, y in mini_batch:
		delta_nabla_b, delta_nabla_w = self.backprop(x, y)
		nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
		nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
	self.weights = [w-(eta/len(mini_batch))*nw 
					for w, nw in zip(self.weights, nabla_w)]
	self.biases = [b-(eta/len(mini_batch))*nb 
					for b, nb in zip(self.biases, nabla_b)]

	return [self.weights, self.biases]


def create_name(learning_rate, reg_level, runs, tgtype, gdstep, cnum):
	
	str_r = 'DBL2m_ETA' + str(learning_rate) +'_reg_levelC' + str(reg_level) + '_GDsteps'+ repr(gdstep) +'_Nruns' +str(runs) + tgtype + '_' + repr(cnum) +'NewIC4times'

	return str_r


"""end of function specification"""





# Here are some parameters for learning and target function specification

#learning_eta = float(sys.argv[1])  # argument i in the batch file
#regularization = float(sys.argv[2]) # argument j in the batch file
#learning_eta = 0.025 
regularization = 0.0
learning_eta = 0.1
NGD= 200 # Gradient descent steps






# File I/O
dir_name='./'
case_num = 2 # for data manipulation 1 for uniform mu 


# First specify the initial binding affinities
Nproteins = 5
wrong_binding = 2.0  # This can be incorporated into learning

Pinput1 = (np.array([[0.9],[0.1]]), np.array([[0.78],[0.22]]),np.array([[0.5],[0.5]]),np.array([[0.2],[0.8]]))
Pinput2 = (np.array([[0.9],[0.1]]), np.array([[0.78],[0.22]]),np.array([[0.5],[0.5]]),np.array([[0.2],[0.8]]))

Ebind = -5.0 + np.zeros((1,5), dtype = float)
#Ebind = np.random.uniform(-6, -5, size = 3)
Ebindall = np.zeros((NGD+1, 4, len(Pinput1)), dtype = float)
MI = np.zeros((NGD, len(Pinput1)), dtype = float)




# here for chemical potential mu_i = - log omega_i
# for uniform case, set omega3= Nproteins
omegavec = np.zeros(Nproteins) + Nproteins  # homogenous
mu = - np.log(omegavec) # chemical potential


marginal4 = np.zeros([2,1]) # output
marginal5 = np.zeros([2,1])



# Start main loop



for inputProb in xrange(len(Pinput1)):

	Ebindall[0,:, inputProb] = Ebind


	for ind in xrange(NGD):

		G = Graph()
		x1 = G.addVarNode('x1',2)
		x2 = G.addVarNode('x2',2)
		x3 = G.addVarNode('x3',2)
		x4 = G.addVarNode('x4',2)
		x5 = G.addVarNode('x5',2)
	

		# Pa3 encodes (x1,x2)=(0,0), (0,1), (1,0), (1,1)

		Pa3 = np.array(list(itertools.product([[0],[1]], repeat = 2))) # repeat= |pa(x3)| 
		Pa4 = np.array([[0],[1]]) 
		Pa5 = np.array([[0],[1]])
			

		#P1 = np.array([[0.8],[0.2]])  # p(x1)=[p(x1=0), p(x1=1)]
		P1 = Pinput1[inputProb] # assuming flat prior
		G.addFacNode(P1, x1)

		#P2 = np.array([[0.7],[0.3]])  # p(x2)
		P2 = Pinput2[inputProb] # assuming flat prior
		G.addFacNode(P2, x2)



		E3 = Ebindall[ind, 0:2, inputProb] # i.e. E13 = -1, E23 = -5
		E4 = np.array([Ebindall[ind, 2, inputProb]])  # i.e. E34 = -1
		E5 = np.array([Ebindall[ind, 3, inputProb]])  # i.e. E35 = -1

			
		# note the syntax: conditional_prob_on(PAstate, Ematrix, Wbind, omega):


		#P34=p(x4|x3) [[x4 OFF when x3=0, x4 ON when x3=0], [x4 OFF when x3=1, x4 ON when x3=1]]
		P34 = np.array([[1-conditional_prob_on(Pa4[0], E4, wrong_binding, omegavec[2]), conditional_prob_on(Pa4[0], E4, wrong_binding, omegavec[2])]\
		,[1-conditional_prob_on(Pa4[1], E4, wrong_binding, omegavec[2]), conditional_prob_on(Pa4[1], E4, wrong_binding, omegavec[2])]])

		G.addFacNode(P34, x3, x4) 

		#P34=p(x5|x3) [[x4 OFF when x3=0, x5 ON when x3=0], [x5 OFF when x3=1, x5 ON when x3=1]]
		P35 = np.array([[1-conditional_prob_on(Pa5[0], E5, wrong_binding, omegavec[2]), conditional_prob_on(Pa5[0], E5, wrong_binding, omegavec[2])]\
		,[1-conditional_prob_on(Pa5[1], E5, wrong_binding, omegavec[2]), conditional_prob_on(Pa5[1], E5, wrong_binding, omegavec[2])]])

		G.addFacNode(P35, x3, x5)



		P123 = np.zeros((2,2,2)) # p(x3|x1 x2)
		P123[:,0,0]=[1-conditional_prob_on(Pa3[0], E3, wrong_binding, omegavec[0:2]), conditional_prob_on(Pa3[0], E3, wrong_binding, omegavec[0:2])]
		P123[:,0,1]=[1-conditional_prob_on(Pa3[1], E3, wrong_binding, omegavec[0:2]), conditional_prob_on(Pa3[1], E3, wrong_binding, omegavec[0:2])]
		P123[:,1,0]=[1-conditional_prob_on(Pa3[2], E3, wrong_binding, omegavec[0:2]), conditional_prob_on(Pa3[2], E3, wrong_binding, omegavec[0:2])]
		P123[:,1,1]=[1-conditional_prob_on(Pa3[3], E3, wrong_binding, omegavec[0:2]), conditional_prob_on(Pa3[3], E3, wrong_binding, omegavec[0:2])]




		G.addFacNode(P123,x3,x1,x2)

		marg = G.marginals()
		distx1 = marg['x1']
		distx2 = marg['x2']
		distx3 = marg['x3']
		distx4 = marg['x4']
		distx5 = marg['x5']
			
		marginal4[0]= float(distx4[0]) # x4 off
		marginal4[1]= float(distx4[1]) # x4 on

		marginal5[0]= float(distx5[0]) # x5 off
		marginal5[1]= float(distx5[1]) # x5 on

		

		# Now construct P124: P(x4|x1 x2), to do so, we need to merge P(x4|x3) and P(x3|x1 x2)

		P124 = np.zeros((2,2,2)) # P(x4|x1 x2)
		P125 = np.zeros((2,2,2)) # P(x5|x1 x2)
		P412joint = np.zeros((2,2,2)) # P(x4 x1 x2)
		P512joint = np.zeros((2,2,2)) # P(x5 x1 x2)
		P45cond12 = np.zeros((2,2,2,2)) # P(x4 x5| X1 X2)
		P1245 = np.zeros((2,2,2,2)) # P(x4 x5 x1 x2)


		P12 = np.outer(P1,P2)
		tmp = np.array(list(itertools.product([[0],[1]], repeat = 2)))
		for i in xrange(tmp.shape[0]):
			i1 = tmp[i][0][0] # index for x1
			i2 = tmp[i][1][0] # index for x2
			P124[0, i1, i2] = P34[0,0]*P123[0,i1,i2]+ P34[1,0]*P123[1,i1,i2]
			P124[1, i1, i2] = P34[0,1]*P123[0,i1,i2]+ P34[1,1]*P123[1,i1,i2]
			P412joint[0,i1,i2] = P124[0,i1,i2]*P12[i1,i2] # only used when comparing BP vs brute force
			P412joint[1,i1,i2] = P124[1,i1,i2]*P12[i1,i2] # only used when comparing BP vs brute force

			P125[0, i1, i2] = P35[0,0]*P123[0,i1,i2]+ P35[1,0]*P123[1,i1,i2]
			P125[1, i1, i2] = P35[0,1]*P123[0,i1,i2]+ P35[1,1]*P123[1,i1,i2]
			P512joint[0,i1,i2] = P125[0,i1,i2]*P12[i1,i2]  # only used when comparing BP vs brute force
			P512joint[1,i1,i2] = P125[1,i1,i2]*P12[i1,i2] # only used when comparing BP vs brute force

			P45cond12[0,0, i1, i2] = P34[0,0]*P123[0,i1,i2]*P35[0,0]+ P34[1,0]*P123[1,i1,i2]*P35[1,0]
			P45cond12[0,1, i1, i2] = P34[0,0]*P123[0,i1,i2]*P35[0,1]+ P34[1,0]*P123[1,i1,i2]*P35[1,1]
			P45cond12[1,0, i1, i2] = P34[0,1]*P123[0,i1,i2]*P35[0,0]+ P34[1,1]*P123[1,i1,i2]*P35[1,0]
			P45cond12[1,1, i1, i2] = P34[0,1]*P123[0,i1,i2]*P35[0,1]+ P34[1,1]*P123[1,i1,i2]*P35[1,1]

			P1245[0,0,i1,i2] = P45cond12[0,0,i1,i2]*P12[i1,i2]
			P1245[0,1,i1,i2] = P45cond12[0,1,i1,i2]*P12[i1,i2]
			P1245[1,0,i1,i2] = P45cond12[1,0,i1,i2]*P12[i1,i2]
			P1245[1,1,i1,i2] = P45cond12[1,1,i1,i2]*P12[i1,i2]




		P45joint = np.zeros((2,2)) 
		P45joint = np.sum(np.sum(P1245, axis = 2), axis = 2) # P(x4, x5)

		P4= np.sum(np.sum(P412joint, axis = 1), axis = 1)
		P5= np.sum(np.sum(P512joint, axis = 1), axis = 1)

	
		
		P124 = P124 + 1e-8 
		P125 = P125 + 1e-8
		P45joint = P45joint + 1e-8
		P45cond12 = P45cond12 + 1e-8


		tmp = np.array(list(itertools.product([[0],[1]], repeat = 2)))
		Ptemp = np.zeros((2,2,2,2)) # term in MI that involves P(out|in), note the notation (x4 x5 x1 x2)
		for i in xrange(tmp.shape[0]):
			i1 = tmp[i][0][0] # index for x1
			i2 = tmp[i][1][0] # index for x2
			Ptemp[:,:, i1, i2] = P45cond12[:,:,i1,i2]*np.log2(P45cond12[:,:,i1,i2]/P45joint)
		


		Ptemp = np.sum(np.sum(Ptemp, axis = 0), axis = 0) # trace over x4 x5
		MI[ind, inputProb] = np.sum(Ptemp*P12) # multiply by P12 and trace over x1 x2
		#MI[ind, inputProb] = np.dot( np.transpose(P2), np.sum(P1*np.multiply(P124[0,:,:], np.log2(P124[0,:,:]/marginal4[0])) + P1*np.multiply(P124[1,:,:], np.log2(P124[1,:,:]/marginal4[1])), axis = 0))
		Ebindall[ind+1,:, inputProb] = Ebindall[ind, :, inputProb] - learning_eta*InfoMaxGD(P1, P2, P34, P123, P45joint,  Ebindall[ind, :, inputProb], wrong_binding, omegavec, regularization)
		G.reset()
	#print Ebindall[NGD, :, inputProb]		



MImax = np.amax(MI, axis=0)	

print MImax

runtick = np.linspace(1,NGD, num=NGD)
plt.plot(runtick, MI[:,0], runtick, MI[:,1], '--',runtick, MI[:,2], runtick, MI[:,3],  linewidth=3.0)
plt.xlabel('Gradient ascent runs', fontsize = fl+4)
plt.ylabel('Mutual Information $MI(x_{in};\, x_4)$', fontsize = fl+4)
plt.xticks(fontsize = ft+4)
plt.yticks(fontsize = ft+4)
plt.legend([r'$P(x_{{in}}=1)=0.1$', r'$P(x_{{in}}=1)=0.22$', r'$P(x_{{in}}=1)=0.5$', r'$P(x_{{in}}=1)=0.8$'], shadow=True, fancybox=True, loc=(0.5, 0.2), fontsize=14+2)
plt.show()

'''

full_str  = 'InfoMax211LearningCurve'
fig_name = os.path.join(dir_name, full_str )
fig.savefig(fig_name+'.eps', bbox_inches='tight')
#fig.savefig(fig_name+'.eps')
#fig.savefig(fig_name+'.png')
#fig.savefig(fig_name+'.pdf', bbox_inches='tight')
fig.savefig(fig_name+'.pdf')
#fig.savefig(fig_name+'.eps')


'''

''