from graph import Graph
from scipy import stats
import numpy as np
import itertools
import math
import sys
import os as os
import cPickle as pickle
from matplotlib import pyplot as plt
import seaborn





dir_current = os.getcwd()
dir_name = dir_current + '/figure'


# set up figure
fig = plt.figure()
#ax = fig.add_subplot(111)
ft = 15 # tick font size
fl = 16 # label font size
fs_s = 12 # small text font size
ms = 5 # marker size
lw = 3 # line width size



"""
This code shows how to use BP (sum-product alg)+ gradient ascent opt. to 
get the marginals of each kinases. 

New IC:  Ebind[0,:] = np.array([-5,-3,-3,-3]) 
original IC : Ebind[0,:] = np.array([-1,-2,-1,-1.5])

New omega" omega3 = 2, omegarest = 8 (i.e. freq3 =0.5, freqrest =0.125)
original omega: all omega = 4


"""




def conditional_prob_on(PAstate, Ematrix, Wbind, omega):
	
	"""
	The conditional prob is dependent on the biophysical model

	"""
	if isinstance(omega, float):
		omega = np.array([omega])

	omegainv= omega**-1
	num = 0.0
	den = 0.0
	


	for j in range(0,len(Ematrix)):
		num = num + omegainv[j]*np.exp(-Ematrix[j])*PAstate[j]
		den = den + omegainv[j]*(np.exp(-Ematrix[j])*PAstate[j] + np.exp(-Wbind)*(1-PAstate[j]))
	
	return num / (1 + den)




def sample_generate(margDist, Nsamples):
	""" margDist is a 2-tuple of len(margDist)
		This functions returns a sample of data generated by the marg prob specified
	"""
	xk = np.arange(2)
	margsamples = np.zeros([len(margDist), Nsamples]) 
	for tupleindex in xrange(len(margDist)):
		custm = stats.rv_discrete(name='custm', values=(xk, margDist[tupleindex]))
		margsamples[tupleindex, :]= custm.rvs(size= Nsamples)

	return margsamples


def InfoMaxGD(P1, P2, P34, P123, P4, bindingE, bindingW, oall, regC):

	nabla_E = np.zeros(len(bindingE))
	underflowcutoff = 0.001


	freqall= oall**-1
	delP4_3 = np.zeros((2,2), dtype = float)  # d P(x4|x3)/ d epsilon_{34}
	delP3_12_1 = np.zeros((2,2,2), dtype = float) # d P(x3|x1 x2) / d epsilon_{13}
	delP3_12_2 = np.zeros((2,2,2), dtype = float) # d P(x3|x1 x2) / d epsilon_{23}
	f124 = np.zeros((2,2,2), dtype = float)
	g124 = np.zeros((2,2,2), dtype = float)
	h124 = np.zeros((2,2,2), dtype = float)



	for tstep in xrange(1):


		delP4_3[1,0] = (-freqall[2]*np.exp(-bindingE[2])*0.0*(1
				+freqall[2]*(1-0.0)*np.exp(-bindingW)))/((1
				+freqall[2]*np.exp(-bindingE[2])*0.0
				+freqall[2]*np.exp(-bindingW)*(1-0.0))**2)

		delP4_3[1,0] = (-freqall[2]*np.exp(-bindingE[2])*1.0*(1
				+freqall[2]*(1-1.0)*np.exp(-bindingW)))/((1
				+freqall[2]*np.exp(-bindingE[2])*1.0
				+freqall[2]*np.exp(-bindingW)*(1-1.0))**2)

		delP4_3[0,:] = - delP4_3[1,:]



		delP3_12_1[1,0,0] = (-freqall[0]*np.exp(-bindingE[0])*0.0*(1
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*0.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*0.0))**2)


		delP3_12_1[1,1,0] = (-freqall[0]*np.exp(-bindingE[0])*1*(1
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*0.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*0.0))**2)


		delP3_12_1[1,0,1] = (-freqall[0]*np.exp(-bindingE[0])*0.0*(1
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*1.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*1.0))**2)


		delP3_12_1[1,1,1] = (-freqall[0]*np.exp(-bindingE[0])*1.0*(1
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*1.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*1.0))**2)


		delP3_12_1[0,:,:] = - delP3_12_1[1,:,:]




		delP3_12_2[1,0,0] = (-freqall[1]*np.exp(-bindingE[1])*0.0*(1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*0.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*0.0))**2)


		delP3_12_2[1,0,1] = (-freqall[1]*np.exp(-bindingE[1])*1*(1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*1.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*0.0
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*0.0-freqall[1]*1.0))**2)


		delP3_12_2[1,1,0] = (-freqall[1]*np.exp(-bindingE[1])*0.0*(1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*0.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+freqall[1]*np.exp(-bindingE[1])*0.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*0.0))**2)


		delP3_12_2[1,1,1] = (-freqall[1]*np.exp(-bindingE[1])*1.0*(1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*1.0)*np.exp(-bindingW)))/((1
				+freqall[0]*np.exp(-bindingE[0])*1.0
				+freqall[1]*np.exp(-bindingE[1])*1.0
				+np.exp(-bindingW)*(freqall[0]+freqall[1]-freqall[0]*1.0-freqall[1]*1.0))**2)


		delP3_12_2[0,:,:] = - delP3_12_2[1,:,:]
		tmp = np.array(list(itertools.product([[0],[1]], repeat = 2)))

		for j in xrange(tmp.shape[0]):
			i1 = tmp[j][0][0]
			i2 = tmp[j][1][0]
			f124[0,i1,i2] = delP4_3[0,0]*P123[0,i1,i2] + delP4_3[0,1]*P123[1,i1,i2]
			f124[1,i1,i2] = delP4_3[1,0]*P123[0,i1,i2] + delP4_3[1,1]*P123[1,i1,i2]
			
			g124[0,i1,i2] = P34[0,0]*delP3_12_1[0,i1,i2] + P34[1,0]*delP3_12_1[1,i1,i2]
			g124[1,i1,i2] = P34[0,1]*delP3_12_1[0,i1,i2] + P34[1,1]*delP3_12_1[1,i1,i2]

			h124[0,i1,i2] = P34[0,0]*delP3_12_2[0,i1,i2] + P34[1,0]*delP3_12_2[1,i1,i2]
			h124[1,i1,i2] = P34[0,1]*delP3_12_2[0,i1,i2] + P34[1,1]*delP3_12_2[1,i1,i2]



		nabla_E[0]  = np.dot( np.transpose(P2), np.sum(P1*(np.multiply(g124[0,:,:], np.log(P124[0,:,:]/P4[0]))+g124[0,:,:]) + P1*(np.multiply(g124[1,:,:], np.log(P124[1,:,:]/P4[1]))+g124[1,:,:]), axis = 0))-4*regC*nabla_E[0]#*(nabla_E[0]**2+nabla_E[1]**2+nabla_E[2]**2)
		nabla_E[1]  = np.dot( np.transpose(P2), np.sum(P1*(np.multiply(h124[0,:,:], np.log(P124[0,:,:]/P4[0]))+h124[0,:,:]) + P1*(np.multiply(h124[1,:,:], np.log(P124[1,:,:]/P4[1]))+h124[1,:,:]), axis = 0))-4*regC*nabla_E[1]#*(nabla_E[0]**2+nabla_E[1]**2+nabla_E[2]**2)	
		nabla_E[2]  = np.dot( np.transpose(P2), np.sum(P1*(np.multiply(f124[0,:,:], np.log(P124[0,:,:]/P4[0]))+f124[0,:,:]) + P1*(np.multiply(f124[1,:,:], np.log(P124[1,:,:]/P4[1]))+f124[1,:,:]), axis = 0))-4*regC*nabla_E[2]#*(nabla_E[0]**2+nabla_E[1]**2+nabla_E[2]**2)

	return nabla_E
			



def SGD(self, training_data, epochs, mini_batch_size, eta,
			test_data=None):
	"""Train the neural network using mini-batch stochastic
	gradient descent.  The ``training_data`` is a list of tuples
	``(x, y)`` representing the training inputs and the desired
	outputs.  The other non-optional parameters are
	self-explanatory.  If ``test_data`` is provided then the
	network will be evaluated against the test data after each
	epoch, and partial progress printed out.  This is useful for
	tracking progress, but slows things down substantially."""
	if test_data: n_test = len(test_data)
	n = len(training_data)
	for j in xrange(epochs):
		random.shuffle(training_data)
		mini_batches = [
			training_data[k:k+mini_batch_size]
			for k in xrange(0, n, mini_batch_size)]
		for mini_batch in mini_batches:
			[W, allbias] = self.update_mini_batch(mini_batch, eta)
		if test_data:
			print "Epoch {0}: {1} / {2}".format(
				j, self.evaluate(test_data), n_test)
		else:
			print "Epoch {0} complete".format(j)
	return [W, allbias]



def update_mini_batch(self, mini_batch, eta):

	"""Update the network's weights and biases by applying
	gradient descent using backpropagation to a single mini batch.
	The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
	is the learning rate."""
	nabla_b = [np.zeros(b.shape) for b in self.biases]
	nabla_w = [np.zeros(w.shape) for w in self.weights]
	for x, y in mini_batch:
		delta_nabla_b, delta_nabla_w = self.backprop(x, y)
		nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
		nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
	self.weights = [w-(eta/len(mini_batch))*nw 
					for w, nw in zip(self.weights, nabla_w)]
	self.biases = [b-(eta/len(mini_batch))*nb 
					for b, nb in zip(self.biases, nabla_b)]

	return [self.weights, self.biases]


def create_name(learning_rate, reg_level, runs, tgtype, gdstep, cnum):
	
	str_r = 'DBL2m_ETA' + str(learning_rate) +'_reg_levelC' + str(reg_level) + '_GDsteps'+ repr(gdstep) +'_Nruns' +str(runs) + tgtype + '_' + repr(cnum) +'NewIC4times'

	return str_r


"""end of function specification"""





# Here are some parameters for learning and target function specification

#learning_eta = float(sys.argv[1])  # argument i in the batch file
#regularization = float(sys.argv[2]) # argument j in the batch file
#learning_eta = 0.025 
regularization = 0.0
learning_eta = 0.02
NGD= 100 # Gradient descent steps






# File I/O
dir_name='./'
case_num = 2 # for data manipulation 1 for uniform mu 


# First specify the initial binding affinities
Nproteins = 4
wrong_binding = 2.0  # This can be incorporated into learning

Pinput1 = (np.array([[0.9],[0.1]]), np.array([[0.78],[0.22]]),np.array([[0.5],[0.5]]),np.array([[0.2],[0.8]]))
Pinput2 = (np.array([[0.9],[0.1]]), np.array([[0.78],[0.22]]),np.array([[0.5],[0.5]]),np.array([[0.2],[0.8]]))

Ebind = -5.0 + np.zeros((1,3), dtype = float)
#Ebind = np.random.uniform(-6, -5, size = 3)
Ebindall = np.zeros((NGD+1, 3, len(Pinput1)), dtype = float)
MI = np.zeros((NGD, len(Pinput1)), dtype = float)




# here for chemical potential mu_i = - log omega_i
# for uniform case, set omega3= Nproteins
omegavec = np.zeros(Nproteins) + Nproteins  # homogenous
mu = - np.log(omegavec) # chemical potential


marginal4 = np.zeros([2,1]) # output



# Start main loop



for inputProb in xrange(len(Pinput1)):

	Ebindall[0,:, inputProb] = Ebind


	for ind in xrange(NGD):

		G = Graph()
		x1 = G.addVarNode('x1',2)
		x2 = G.addVarNode('x2',2)
		x3 = G.addVarNode('x3',2)
		x4 = G.addVarNode('x4',2)
	

		# Pa3 encodes (x1,x2)=(0,0), (0,1), (1,0), (1,1)

		Pa3 = np.array(list(itertools.product([[0],[1]], repeat = 2))) # repeat= |pa(x3)| 
		Pa4 = np.array([[0],[1]]) 
			

		#P1 = np.array([[0.8],[0.2]])  # p(x1)=[p(x1=0), p(x1=1)]
		P1 = Pinput1[inputProb] # assuming flat prior
		G.addFacNode(P1, x1)

		#P2 = np.array([[0.7],[0.3]])  # p(x2)
		P2 = Pinput2[inputProb] # assuming flat prior
		G.addFacNode(P2, x2)



		E3 = Ebindall[ind, 0:2, inputProb] # i.e. E13 = -1, E23 = -5
		E4 = np.array([Ebindall[ind, 2, inputProb]])  # i.e. E34 = -1
			
		# note the syntax: conditional_prob_on(PAstate, Ematrix, Wbind, omega):


		#P34=p(x4|x3) [[x4 OFF when x3=0, x4 ON when x3=0], [x4 OFF when x3=1, x4 ON when x3=1]]
		P34 = np.array([[1-conditional_prob_on(Pa4[0], E4, wrong_binding, omegavec[2]), conditional_prob_on(Pa4[0], E4, wrong_binding, omegavec[2])]\
		,[1-conditional_prob_on(Pa4[1], E4, wrong_binding, omegavec[2]), conditional_prob_on(Pa4[1], E4, wrong_binding, omegavec[2])]])

		G.addFacNode(P34, x3, x4) 


		#P123= p(x3|x1,x2)

		"""
		The probability of x3 given its parents. Note that this is in fact of 
		conditional probability table (CPT). One example of such table is:

		P123=np.array([[[0.8,0.75],[0.83,0.87]],[[0.2,0.25],[0.17,0.13]]])

		P123[0,i,j]+P123[1,i,j]=1 for all i,j
		"""



		P123 = np.zeros((2,2,2)) # p(x3|x1 x2)
		P123[:,0,0]=[1-conditional_prob_on(Pa3[0], E3, wrong_binding, omegavec[0:2]), conditional_prob_on(Pa3[0], E3, wrong_binding, omegavec[0:2])]
		P123[:,0,1]=[1-conditional_prob_on(Pa3[1], E3, wrong_binding, omegavec[0:2]), conditional_prob_on(Pa3[1], E3, wrong_binding, omegavec[0:2])]
		P123[:,1,0]=[1-conditional_prob_on(Pa3[2], E3, wrong_binding, omegavec[0:2]), conditional_prob_on(Pa3[2], E3, wrong_binding, omegavec[0:2])]
		P123[:,1,1]=[1-conditional_prob_on(Pa3[3], E3, wrong_binding, omegavec[0:2]), conditional_prob_on(Pa3[3], E3, wrong_binding, omegavec[0:2])]




		G.addFacNode(P123,x3,x1,x2)

		marg = G.marginals()
		distx1 = marg['x1']
		distx2 = marg['x2']
		distx3 = marg['x3']
		distx4 = marg['x4']
			
		marginal4[0]= float(distx4[0]) # x4 off
		marginal4[1]= float(distx4[1]) # x4 on
		

		# Now construct P124: P(x4|x1 x2), to do so, we need to merge P(x4|x3) and P(x3|x1 x2)

		P124 = np.zeros((2,2,2)) # P(x4|x1 x2)
		tmp = np.array(list(itertools.product([[0],[1]], repeat = 2)))
		for i in xrange(tmp.shape[0]):
			i1 = tmp[i][0][0] # index for x1
			i2 = tmp[i][1][0] # index for x2
			P124[0, i1, i2] = P34[0,0]*P123[0,i1,i2]+ P34[1,0]*P123[1,i1,i2]
			P124[1, i1, i2] = P34[0,1]*P123[0,i1,i2]+ P34[1,1]*P123[1,i1,i2]


		P12 = np.outer(P1,P2)
		P412joint = np.zeros((2,2,2))
		tmp = np.array(list(itertools.product([[0],[1]], repeat = 2)))

		for i in xrange(tmp.shape[0]):
			i1 = tmp[i][0][0]
			i2 = tmp[i][1][0]
			P412joint[0,i1,i2] = P124[0,i1,i2]*P12[i1,i2]
			P412joint[1,i1,i2] = P124[1,i1,i2]*P12[i1,i2]

		P4= np.sum(np.sum(P412joint, axis = 1), axis = 1)
		#print 'P4 brute force='
		#print P4
		#marginal4 = np.array(P4)
		
		P124 = P124 + 1e-8 
		


		MI[ind, inputProb] = np.dot( np.transpose(P2), np.sum(P1*np.multiply(P124[0,:,:], np.log2(P124[0,:,:]/marginal4[0])) + P1*np.multiply(P124[1,:,:], np.log2(P124[1,:,:]/marginal4[1])), axis = 0))
		Ebindall[ind+1,:, inputProb] = Ebindall[ind, :, inputProb] - learning_eta*InfoMaxGD(P1, P2, P34, P123, marginal4,  Ebindall[ind, :, inputProb], wrong_binding, omegavec, regularization)
		G.reset()
	print Ebindall[NGD, :, inputProb]		



MImax = np.amax(MI, axis=0)	

print MImax
#print Ebindall[NGD, :, len(Pinput1)-1]

#print MI.shape
#print Ebindall

runtick = np.linspace(1,NGD, num=NGD)
plt.plot(runtick, MI[:,0], runtick, MI[:,1], '--',runtick, MI[:,2], runtick, MI[:,3],  linewidth=3.0)
plt.xlabel('Gradient ascent runs', fontsize = fl+4)
plt.ylabel('Mutual Information $MI(x_{in};\, x_4)$', fontsize = fl+4)
plt.xticks(fontsize = ft+4)
plt.yticks(fontsize = ft+4)
plt.legend([r'$P(x_{{in}}=1)=0.1$', r'$P(x_{{in}}=1)=0.22$', r'$P(x_{{in}}=1)=0.5$', r'$P(x_{{in}}=1)=0.8$'], shadow=True, fancybox=True, loc=(0.5, 0.2), fontsize=14+2)
plt.show()

'''

full_str  = 'InfoMax211LearningCurve'
fig_name = os.path.join(dir_name, full_str )
fig.savefig(fig_name+'.eps', bbox_inches='tight')
#fig.savefig(fig_name+'.eps')
#fig.savefig(fig_name+'.png')
#fig.savefig(fig_name+'.pdf', bbox_inches='tight')
fig.savefig(fig_name+'.pdf')
#fig.savefig(fig_name+'.eps')


'''

